{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../local/az_reduced_reviews.csv')\n",
    "az_biz = pd.read_csv('../local/az_biz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbarranger/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "x_df = df[df.stars != 3]\n",
    "#ignore warning\n",
    "x_df['binary_stars'] =  np.where(x_df['stars'] >= 4, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_df.text, x_df.binary_stars, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9080282515498257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick pipeline to show best performing nb\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),  \n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)\n",
    "#90.61% accurate - w/o stopword filter\n",
    "#90.80% accurate stop_words='english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbarranger/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8747679116676433"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick pipeline to show best performing svm\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                         ('tfidf', TfidfTransformer()), \n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)),\n",
    "                        ])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm == y_test)\n",
    "# 89.266% accurate\n",
    "# 87.47% accurate stop_words='english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(648456, 144542)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorize the vocab\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "X_train_vec = CountVectorizer().fit(X_train)\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(648456, 144542)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf the vectorized corpus\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train a NB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative features:\n",
      "  rowed (-3.99)\n",
      "  display (-3.98)\n",
      "  visted (-3.95)\n",
      "  slilghtly (-3.94)\n",
      "  uge (-3.93)\n",
      "  woo (-3.82)\n",
      "  pnash (-3.75)\n",
      "  coche (-3.73)\n",
      "  rainbows (-3.73)\n",
      "  inbox (-3.68)\n",
      "\n",
      "Most positive features:\n",
      "  youbuywefly (2.90)\n",
      "  honeybee (2.91)\n",
      "  scraggly (2.93)\n",
      "  deleterious (2.95)\n",
      "  planters (2.96)\n",
      "  ewwwwww (2.96)\n",
      "  peppe (3.00)\n",
      "  delictible (3.00)\n",
      "  addictd (3.06)\n",
      "  geeeeeeeeeee (3.38)\n"
     ]
    }
   ],
   "source": [
    "#look at the top indicating positive and negative terms\n",
    "linear_weights = nb.feature_log_prob_[1,] - nb.feature_log_prob_[0,]  # populate this with actual values\n",
    "\n",
    "top_negative_features= np.argsort(linear_weights)[0:10]\n",
    "top_positive_features= np.argsort(linear_weights)[-10:]\n",
    "\n",
    "print(\"Most negative features:\")\n",
    "for idx in top_negative_features:\n",
    "    for k, v in X_train_vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(\"  {:s} ({:.02f})\".format(k, linear_weights[idx]))\n",
    "            \n",
    "print(\"\")\n",
    "print(\"Most positive features:\")\n",
    "for idx in top_positive_features:\n",
    "    for k, v in X_train_vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(\"  {:s} ({:.02f})\".format(k, linear_weights[v]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view az data by business with most reviews\n",
    "az_biz.sort_values('review_count', ascending=False)\n",
    "# pSQFynH1VxkfSmehRXlZWw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative features:\n",
      "  rude (-1.81)\n",
      "  overrated (-1.69)\n",
      "  mediocre (-1.58)\n",
      "  worst (-1.51)\n",
      "  burned (-1.50)\n",
      "  overpriced (-1.44)\n",
      "  disappointment (-1.39)\n",
      "  attitude (-1.39)\n",
      "  tasteless (-1.37)\n",
      "  waste (-1.31)\n",
      "\n",
      "Most positive features:\n",
      "  margherita (1.71)\n",
      "  excellent (1.78)\n",
      "  rosa (1.80)\n",
      "  fresh (1.83)\n",
      "  loved (1.87)\n",
      "  favorite (1.90)\n",
      "  fantastic (1.97)\n",
      "  amazing (2.00)\n",
      "  perfect (2.07)\n",
      "  delicious (2.24)\n"
     ]
    }
   ],
   "source": [
    "#run the same pipeline on a single business\n",
    "single_business = x_df[x_df['business_id'] == 'pSQFynH1VxkfSmehRXlZWw']\n",
    "sb_x = single_business['text']\n",
    "y_sb = single_business['binary_stars']\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "sb_xtrain_vec = CountVectorizer().fit(sb_x)\n",
    "sb_xtrain_counts = count_vect.fit_transform(sb_x)\n",
    "sb_xtrain_counts.shape\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "sb_xtrain_tfidf = tfidf_transformer.fit_transform(sb_xtrain_counts)\n",
    "sb_xtrain_tfidf.shape\n",
    "\n",
    "nb_sb = MultinomialNB()\n",
    "nb_sb.fit(sb_xtrain_tfidf, y_sb)\n",
    "\n",
    "linear_weights_sb = nb_sb.feature_log_prob_[1,] - nb_sb.feature_log_prob_[0,]  # populate this with actual values\n",
    "\n",
    "top_negative_features= np.argsort(linear_weights_sb)[0:10]\n",
    "top_positive_features= np.argsort(linear_weights_sb)[-10:]\n",
    "\n",
    "print(\"Most negative features:\")\n",
    "for idx in top_negative_features:\n",
    "    for k, v in sb_xtrain_vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(\"  {:s} ({:.02f})\".format(k, linear_weights_sb[v]))\n",
    "            \n",
    "print(\"\")\n",
    "print(\"Most positive features:\")\n",
    "for idx in top_positive_features:\n",
    "    for k, v in sb_xtrain_vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(\"  {:s} ({:.02f})\".format(k, linear_weights_sb[v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "diff_dict = {}\n",
    "top_n_positive_features= np.argsort(linear_weights_sb)[-n:]\n",
    "\n",
    "for idx in top_n_positive_features:\n",
    "    for a, b in sb_xtrain_vec.vocabulary_.items(): \n",
    "        if b == idx:\n",
    "            for j, k in X_train_vec.vocabulary_.items():\n",
    "                if j == a:\n",
    "                    diff = linear_weights_sb[b]- linear_weights[k]\n",
    "                    diff_dict[j] = diff\n",
    "        #                 print(j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 1.257478043232216),\n",
       " ('arugula', 1.6098550582885425),\n",
       " ('atmosphere', 1.846626801427238),\n",
       " ('next', 1.9510481249161176),\n",
       " ('best', 1.9998315525797485),\n",
       " ('pistachios', 2.058481664986666),\n",
       " ('awesome', 2.102141502290209),\n",
       " ('enjoy', 2.161907336691808),\n",
       " ('love', 2.2049764736315716),\n",
       " ('margherita', 2.2129599782474045),\n",
       " ('prepared', 2.224162370457762),\n",
       " ('sonny', 2.2666705777721354),\n",
       " ('wise', 2.370966307213438),\n",
       " ('great', 2.4266732470190906),\n",
       " ('biancoverde', 2.429987559367463),\n",
       " ('rosa', 2.4426300311379316),\n",
       " ('fresh', 2.4478185611876153),\n",
       " ('favorite', 2.5144553179313327),\n",
       " ('delicious', 2.532255901685552),\n",
       " ('enjoyed', 2.6238846800963085),\n",
       " ('amazing', 2.653061479834234),\n",
       " ('mozzarella', 2.6903166750243486),\n",
       " ('fantastic', 2.7703076335011474),\n",
       " ('perfect', 2.8884457652947795),\n",
       " ('tasty', 2.9117169378011596)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "sorted_diff_dict = sorted(diff_dict.items(), key=operator.itemgetter(1))\n",
    "sorted_diff_dict[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice area below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_biz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_positive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in top_positive_features:\n",
    "    for k, v in vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(k)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
