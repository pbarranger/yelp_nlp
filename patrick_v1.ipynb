{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../local/az_reduced_reviews.csv')\n",
    "az_biz = pd.read_csv('../local/az_biz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbarranger/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "x_df = df[df.stars != 3]\n",
    "#ignore warning\n",
    "x_df['binary_stars'] =  np.where(x_df['stars'] >= 4, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_df.text, x_df.binary_stars, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9080282515498257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick pipeline to show best performing nb\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),  \n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)\n",
    "#90.61% accurate - w/o stopword filter\n",
    "#90.80% accurate stop_words='english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbarranger/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8747679116676433"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick pipeline to show best performing svm\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                         ('tfidf', TfidfTransformer()), \n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)),\n",
    "                        ])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm == y_test)\n",
    "# 89.266% accurate\n",
    "# 87.47% accurate stop_words='english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the vocab\n",
    "count_vect = CountVectorizer()\n",
    "X_train_vec = CountVectorizer().fit(X_train)\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf the vectorized corpus\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a NB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the top indicating positive and negative terms\n",
    "linear_weights = nb.feature_log_prob_[1,] - nb.feature_log_prob_[0,]  # populate this with actual values\n",
    "\n",
    "top_negative_features= np.argsort(linear_weights)[0:10]\n",
    "top_positive_features= np.argsort(linear_weights)[-10:]\n",
    "\n",
    "print(\"Most negative features:\")\n",
    "for idx in top_negative_features:\n",
    "    for k, v in X_train_vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(\"  {:s} ({:.02f})\".format(k, linear_weights[idx]))\n",
    "            \n",
    "print(\"\")\n",
    "print(\"Most positive features:\")\n",
    "for idx in top_positive_features:\n",
    "    for k, v in X_train_vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(\"  {:s} ({:.02f})\".format(k, linear_weights[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view az data by business with most reviews\n",
    "az_biz.sort_values('review_count', ascending=False)\n",
    "# pSQFynH1VxkfSmehRXlZWw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative features:\n",
      "  rude (-1.81)\n",
      "  overrated (-1.69)\n",
      "  mediocre (-1.58)\n",
      "  worst (-1.51)\n",
      "  burned (-1.50)\n",
      "  overpriced (-1.44)\n",
      "  disappointment (-1.39)\n",
      "  attitude (-1.39)\n",
      "  tasteless (-1.37)\n",
      "  waste (-1.31)\n",
      "\n",
      "Most positive features:\n",
      "  margherita (1.71)\n",
      "  excellent (1.78)\n",
      "  rosa (1.80)\n",
      "  fresh (1.83)\n",
      "  loved (1.87)\n",
      "  favorite (1.90)\n",
      "  fantastic (1.97)\n",
      "  amazing (2.00)\n",
      "  perfect (2.07)\n",
      "  delicious (2.24)\n"
     ]
    }
   ],
   "source": [
    "#run the same pipeline on a single business\n",
    "single_business = x_df[x_df['business_id'] == 'pSQFynH1VxkfSmehRXlZWw']\n",
    "sb_x = single_business['text']\n",
    "y_sb = single_business['binary_stars']\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "sb_xtrain_vec = CountVectorizer().fit(sb_x)\n",
    "sb_xtrain_counts = count_vect.fit_transform(sb_x)\n",
    "sb_xtrain_counts.shape\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "sb_xtrain_tfidf = tfidf_transformer.fit_transform(sb_xtrain_counts)\n",
    "sb_xtrain_tfidf.shape\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(sb_xtrain_tfidf, y_sb)\n",
    "\n",
    "linear_weights = nb.feature_log_prob_[1,] - nb.feature_log_prob_[0,]  # populate this with actual values\n",
    "\n",
    "top_negative_features= np.argsort(linear_weights)[0:10]\n",
    "top_positive_features= np.argsort(linear_weights)[-10:]\n",
    "\n",
    "print(\"Most negative features:\")\n",
    "for idx in top_negative_features:\n",
    "    for k, v in sb_xtrain_vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(\"  {:s} ({:.02f})\".format(k, linear_weights[idx]))\n",
    "            \n",
    "print(\"\")\n",
    "print(\"Most positive features:\")\n",
    "for idx in top_positive_features:\n",
    "    for k, v in sb_xtrain_vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(\"  {:s} ({:.02f})\".format(k, linear_weights[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice area below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_biz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_positive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in top_positive_features:\n",
    "    for k, v in vec.vocabulary_.items():\n",
    "        if v == idx:\n",
    "            print(k)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
